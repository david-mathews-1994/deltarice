# Optimization of M and the Encoding Filter

The optimization of M and the Encoding Filter can be done independently, or in tandem. Each is discussed independently here, but the simple routine presented within this repository is capable of minimizing both in tandem.

## Optimization of M

Optimization of $m$ requires knowledge of the probability density function of values after preparatory encoding, $P(x)$. Determining of $P(x)$ can be done either on a per-file basis which will provide the best possible compression from this method at the expense of throughput, or the assumption can be made that $P(x)$ does not change significantly from file to file in a dataset and can be determined once for a subset of the data. The optimal value for $m$ is found via minimization of $$ B(m, c) = \sum_i P(x_i) * b(x_i, m, c, b_0) $$ where $b(x_i, m, c, b_0)$ is the number of bits required for each datapoint and $b_0$ is the number of bits used in the original representation of each value $x_i$. The sum is over all possible values of $x_i$ as defined by the finite range of digitized values. For example, $x_i$ ranges from $0$ to $16383$ for 14 bit digitizers. For the case of a Gaussian probability distribution $P(x)$ centered about $0$, the relationship between the standard deviation $\sigma$ and $m$ is explored in Figure \ref{fig:CorrelationPlot}. Note that datasets with smaller standard deviations will compress better than larger standard deviation datasets, but are also more sensitive to the choice of $m$ parameter.

![](./images/CompressionRatioPlotCropped.png?raw=true)
*Compression Ratio as a function of $m$ and $\sigma$ for Gaussian distributions using Rice Coding. Locations marked with $\star$ represent the best compression for that $\sigma$ value. The values of $x$ ranged from $-8192$ to $8192$ as if from a signed 14 bit digitizer.*


## Optimization of the Encoding Filter
Determining the optimal encoding filter is a non-trivial minimization process with a strict set of rules that must be followed. First, any encoding filter must be entirely comprised of only integer values because Rice coding is designed for integer values. The filter should also not have zeros on either end. A zero at the end of the filter does not affect the distribution of values being fed into the Rice coding operation, except in the null case of a filter only defined with zeros such as $[0]$ where the data is destroyed, and results in needing to store a longer encoding filter alongside the compressed data which is undesirable. 

The routine provided with this software starts with the user providing an initial guess as to the best encoding filter. Only filters of the same length, $n$, as the initial filter will be tested by this routine. Alternate filters in the surrounding space of the initial guess are then tested on the dataset that the user passes to the routine to see which produces the smallest average number of bits per input value. If the filter with the smallest number of bits is the initial filter for this step, then the routine stops, otherwise the initial guess is updated with the current best filter. This process repeats until the initial guess is the filter with the smallest average number of bits per input value within the current region being tested. This algorithm stores previously calculated average bit sizes in case the algorithm checks the same filter definition multiple times to prevent redundant calculations.

The determination of the neighboring filters is done by shifting each value in the initial filter up or down by up-to a user defined search range parameter $s$. For example, for an input filter of $[2, -2]$ and $s=1$, then 8 surrounding filters would be checked for all possible combinations with the first element spanning $1, 2, 3$ and the second element spanning $-3, -2, -1$, except for the repeated filter of $[2, -2]$. By increasing the search range parameter $s$ the chance of this simplistic optimization routine falling into a local minima is reduced, but the number of filters checked is greatly increased. The number of filters checked at each step by this minimization routine scales as $(2s+1)^n$ with $s$ being the search parameter, ignoring invalid filters such as those with 0's at the end or already-checked filters, so keeping the length of the filter short is important if this routine is to be used.

It is important to note that the encoding operation will take more time as the filter length increases due to the convolution operation. In addition, a longer filter will also require more storage as it must be stored alongside the compressed data in order for the decompression operation to function, but for large datasets this contribution to the output size is negligible. A comparison between filter length, compression performance, and encoding time is shown in below for data from the Nab experiment. Delta encoding offers the highest throughput, other than not encoding at all, while also offering the best compression performance of the filters tested so the choice in encoding filter for Nab was simple. In testing on preliminary data from the NOPTREX collaboration, the encoding filter $[1, -1, 1, -1]$ reduced the data size $14\%$ further than delta encoding, but took $\approx 8\times$ longer to perform the encoding. Depending on the experimental priority, data reduction vs throughput, the decision can be made as to which encoding filter is chosen. If, for example, the reduction in speed from a more complicated encoding filter was not enough to lower the compression throughput below the data acquisition rate, then it may be deemed an acceptable slowdown.

![](./images/SizeReductionAndEncodingTime.png?raw=true)
*Comparing the reduction in size vs the encoding time for a variety of filter lengths on a Nab dataset. While longer filters offered similar compressed sizes as delta encoding, the time taken to encode the data increased significantly with each additional parameter in the filter.*
