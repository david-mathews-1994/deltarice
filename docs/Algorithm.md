# Algorithm Overview
This algorithm is a two-step process: the digitized signal is first passed through an encoding operation, such as delta encoding, to de-correlate the data and prepare it for the second step of Rice coding. These methods were chosen for this compression routine specifically for their simplicity, throughput, and storage efficiency. They also do not require a significant amount of additional information to be stored alongside the compressed data in order for the decompression routine to function, which improves storage efficiency further. In the subsequent sections we describe the Rice Coding operation and the resulting requirements placed on the preparatory encoding filter.

## Rice Coding
The requirements for the preparatory encoding step are driven by the behavior of Rice coding, described here. Rice coding functions by encoding a value $x$ in 2 pieces: $q$, the result of a division by a tunable parameter $m$, and $r$, the remainder of that division. Rice coding expects unsigned values, but can be extended to support signed numbers by interleaving positive and negative values. A straightforward way of accomplishing this is $x\prime = 2*x$ for $x>=0$ and $x\prime = 2|x|-1$ for $x<0$. With $x\prime$ known, q and r can be calculated via $q = x\prime // m$ and $r=x\prime - q * m$ where $//$ represents floor division. These $q$ and $r$ values are then packed together with $q$ stored in Unary coding and $r$ stored in truncated binary encoding. The number of bits required to store the remainder $r$ will depend on the choice of $m$, with the optimal $m$ depending on the features of the data being compressed. Unary coding takes $q+1$ bits to store $q$ while the storage of $r$ takes either $s-1$ or $s$ bits, where $s=\lceil \log_2(m) \rceil$, if $0\leq r<2^{s}-m$ or if $2^{s}-m \leq r < m$ respectively. Note that for Rice coding $m$ is restricted to being a power of $2$ which sets the size of the remainder $r$ to $s$ bits. Additionally, fixing $m$ to a power of $2$ reduces the computational complexity of determining $q$ and $r$ as they can be calculated with bit operations instead of using the division operator that can be particularly problematic on hardware such as FPGAs. For this reason, Rice coding is used in this compression routine instead of the more general form of Golomb coding where $m$ is not restricted to only powers of $2$.

In Rice coding, statistical outliers with large $q$ values can be problematic due to using Unary coding for the quotient $q$ which could expand to being larger than the initial storage size. This should be unlikely in a dataset that is well suited for Rice coding, which is one of the purposes of the preparatory encoding enabled in this algorithm. In the case that this does occur still, a cutoff parameter $c$ is defined in this algorithm to set an upper limit for the value of $q$. In the case that $q>c$, the algorithm outputs the cutoff parameter $c$ in Unary coding followed by the original input value in its original representation. This does expand the storage of statistical outliers, particularly those just beyond the cutoff value, but fixes the upper limit of the storage size. The cutoff parameter $c$ is fixed to $8$ in this implementation.

The outputs from this method are packed sequentially into a temporary 64 bit storage buffer. For example, for a given input parameter $x=-2$ with $m=8$: $x\prime=3$, $q=0$ and $r=3$. This would be expressed as $1011$ which requires 4 bits of storage space. Larger values further from 0 will require more bits such as $x=25$ where $x\prime=50$, $q=6$ and $r=2$. This would be expressed as $0000001010$ requiring 10 bits of storage. Multiple sets of $q$ and $r$ are packed together for output in the order $q_1 r_1 q_2 r_2$ for input $x_1 x_2$. In this routine, the output is formatted as unsigned 32 bit integers. In order to efficiently use that output space, a 64 bit unsigned integer is used as a buffer to concatenate each $qr$ pair into. Once the number of bits used within this buffer reaches 32 bits or greater, the first 32 bits are output and the remaining information within the buffer is bit shifted to the start. For these example values, the buffer would look like $10110000001010$. This buffer is then re-used for the remaining data until all values $x$ have been compressed and output. This ensures that no bits are wasted in the 32 bit unsigned values that are written to file, except for any unused bits in the final 32 bit output. The total number of elements $x$ is written at the beginning of the compressed data chunk so the decompression routine knows how many values to expect, which is critical for stopping the routine when parsing the final output. A small-scale demonstration of this packing is shown in the table below for the above example in the case of a 16 bit buffer and an 8 bit output size. 

| Bit          | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |
|--------------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Initial      | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1st Pack     | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 2nd Pack     | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |
| After Output | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |

This compression routine can be reconstructed by parsing each bit within the 32 bit output counting the zeros until the first 1 is located. The number of preceeding zeros sets the value $q$ and the remainder is defined by the next $s$ bits. In order for the reconstruction routine to know $s$ and how to reconstruct $x$ from $q$ and $r$, the parameter $m$ must be stored alongside the compressed data along with $c$ in applications where the cutoff is not fixed. 

Optimization of both $m$ and $c$ requires knowledge of the probability density function of values after preparatory encoding, $P(x)$. Determining of $P(x)$ can be done either on a per-file basis which will provide the best possible compression from this method at the expense of throughput, or the assumption can be made that $P(x)$ does not change significantly from file to file in a dataset and can be determined once for a subset of the data. The optimal values for $m$ and $c$ are found via minimization of $$ B(m, c) = \sum_i P(x_i) * b(x_i, m, c, b_0) $$ where $b(x_i, m, c, b_0)$ is the number of bits required for each datapoint and $b_0$ is the number of bits used in the original representation of each value $x_i$. The sum is over all possible values of $x_i$ as defined by the finite range of digitized values. For example, $x_i$ ranges from $0$ to $16383$ for 14 bit digitizers. For the case of a Gaussian probability distribution $P(x)$ centered about $0$, the relationship between the standard deviation $\sigma$ and $m$ is explored in Figure \ref{fig:CorrelationPlot}. Note that datasets with smaller standard deviations will compress better than larger standard deviation datasets, but are also more sensitive to the choice of $m$ parameter. In this work, the cutoff value $c$ is fixed to 8, but in general the cutoff value $c$ should be optimized in tandem with $m$. 

![Compression Ratio as a function of $m$ and $\sigma$ for Gaussian distributions using Rice Coding. Locations marked with $\star$ represent the best compression for that $\sigma$ value. No cutoff value $c$ was used during this optimization to prevent the representation of $q$ from expanding past the initial storage size. The values of $x$ ranged from $-8192$ to $8192$ as if from a signed 14 bit digitizer](./images/CompressionRatioPlot.png?raw=true)

## Preparatory Encoding
The goal of this encoding operation is to manipulate the incoming data into a form that is more optimal for Rice Coding. As shown for a Gaussian probability distribution in Figure \ref{fig:CorrelationPlot}, a dataset with a smaller standard deviation can be compressed to a smaller overall size compared to a dataset with a larger standard deviation. This makes intuitive sense as a dataset with a smaller distribution centered around $0$ will have a smaller average value $q$ for a given $m$, thus reducing the output size of the compressed data. In addition to reducing the spread in the data and centering the values around $0$, any encoding operation must be both reversible, so the encoding can be undone when the data is de-compressed, and maintain integer precision, as Rice encoding is defined for integer input values $x$. 

The default encoding operation performed in this routine is delta encoding. This method stores the difference between subsequent values in the dataset instead of the original values. For example, a signal of $-9, 8, -4, 15, 2, 3, 6$ passed through delta encoding results in $-9, 17, -12, 19, -13, 1, 3$. Figure \ref{fig:ApplyingDelta} shows an example of delta encoding applied to a sample waveform trace from the Nab experiment. Note the clear reduction in the standard deviation of the data and the movement of the average value to being near 0, both of which improve the compression of the data with Rice Coding.

Delta encoding may not be the ideal encoding operation for a given dataset so this compression routine supports the use of alternative encoding filters. These alternate filters are implemented via convolution with the original data, which can be performed iteratively or recursively depending on the hardware. These filters are preferably small as the filter must be stored alongside the compressed data to allow for decompression. The functions for encoding and decoding data with an arbitrary filter can be seen in the source code. Delta encoding can also be expressed in terms of a convolutional filter, in which case it is defined as $[1, -1]$. Determination of the optimal encoding filter is discussed in the Optimization.md file within the docs/ folder in the repository.

![Left: A waveform before and after delta encoding. Applying Rice coding with $m=8$ on the original signal expands the size of the waveform from 14 kB to 18.2 kB. The same Rice coding operation on the delta encoded waveform compresses the waveform to 4.6 kB, 33% the original size. Right: A histogram of a sample dataset before and after delta encoding. Note the clear reduction in the distribution width and that the most probable values are centered around 0](./images/ExampleEncoding.png?raw=true)
